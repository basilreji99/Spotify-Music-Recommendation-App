{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b08a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the song: Cruel Summer\n",
      "Recommended tracks for the target track:\n",
      "Titi Me Preguntï¿ - Bad Bunny\n",
      "Nobody Like U - From \"Turning Red\" - Jordan Fisher, Josh Levi, Finneas O'Connell, 4*TOWN (From Disney and Pixarï¿½ï¿½ï¿½s Turning Red), Topher Ngo, Grayson Vill\n",
      "Leave Before You Love Me (with Jonas Brothers) - Marshmello, Jonas Brothers\n",
      "Tere Vaaste (From \"Zara Hatke Zara Bachke\") - Sachin-Jigar, Shadab Faridi, Altamash Faridi, Amitabh Bhattacharya, Varun Jain\n",
      "Grapejuice - Harry Styles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.hussain\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "data = pd.read_csv('C:/Users/s.hussain/Downloads/spotify-2023.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Select relevant features for recommendation\n",
    "# we can cut down what we select to recommend music\n",
    "features = ['energy_%']\n",
    "\n",
    "#map the musical keys to numbers\n",
    "musicalkeystonum = {\n",
    "    'C': 0, 'C#': 1, 'D': 2, 'D#': 3, 'E': 4, 'F': 5, 'F#': 6, 'G': 7, 'G#': 8, 'A': 9, 'A#': 10, 'B': 11\n",
    "}\n",
    "data['key'] = data['key'].map(musicalkeystonum)\n",
    "\n",
    "# mode to numerical values\n",
    "mode_mapping = {'Major': 1, 'Minor': 0}\n",
    "data['mode'] = data['mode'].map(mode_mapping)\n",
    "\n",
    "\n",
    "# drop rows if missing features\n",
    "data = data.dropna(subset=features)\n",
    "\n",
    "#fit the KNN model\n",
    "k = 6 \n",
    "knn = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "knn.fit(data[features], data[features].columns)\n",
    "\n",
    "# User input: name of the song for which recommendations are needed\n",
    "target_track_name = input(\"Enter the name of the song: \")  \n",
    "\n",
    "# find the index of the song because we decoded earlier\n",
    "target_track_index = data[data['track_name'] == target_track_name].index.tolist()\n",
    "if not target_track_index:\n",
    "    print(\"Song not found.\")\n",
    "else:\n",
    "    target_track_index = target_track_index[0]\n",
    "\n",
    "distances, indices = knn.kneighbors([data.iloc[target_track_index][features]])\n",
    "\n",
    "#print recommended tracks\n",
    "print(\"Recommended tracks for the target track:\")\n",
    "for idx in indices[0]:\n",
    "    if idx != target_track_index:  # Exclude the target track itself\n",
    "        recommended_track = data.iloc[idx]\n",
    "        print(recommended_track['track_name'], \"-\", recommended_track['artist(s)_name'])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dcd861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Labels DataFrame:\n",
      "   track_index  true_label        track_name       artist(s)_name\n",
      "0          926           0         Typa Girl            BLACKPINK\n",
      "1          940           1     Sweet Nothing         Taylor Swift\n",
      "2          444           0  Jingle Bell Rock          Bobby Helms\n",
      "3          649           0      Still D.R.E.  Dr. Dre, Snoop Dogg\n",
      "4          889           0         Attention             NewJeans\n",
      "5          105           1      Primera Cita           Carin Leon\n",
      "\n",
      "Predicted Labels DataFrame:\n",
      "   track_index  predicted_label                      track_name  \\\n",
      "0          231              1.0                         CUFF IT   \n",
      "1          290              1.0                       Shut Down   \n",
      "2          159              1.0             Under The Influence   \n",
      "3          107              1.0                      Dandelions   \n",
      "4          146              1.0                   Ojitos Lindos   \n",
      "5          248              1.0  Danger (Spider) (Offset & JID)   \n",
      "\n",
      "              artist(s)_name  \n",
      "0                   Beyoncï¿  \n",
      "1                  BLACKPINK  \n",
      "2                Chris Brown  \n",
      "3                    Ruth B.  \n",
      "4  Bomba Estï¿½ï¿½reo, Bad B  \n",
      "5                Offset, JID  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random true labels for demonstration purposes\n",
    "num_tracks = len(data)\n",
    "true_labels = np.random.choice([0, 1], size=num_tracks)  # Assuming binary labels (0: not relevant, 1: relevant)\n",
    "\n",
    "# Assuming you have already generated recommendations and stored them in a list\n",
    "# For demonstration purposes, let's assume we have randomly generated recommendations\n",
    "num_recommendations = len(indices[0])  # Number of recommended tracks for the target track\n",
    "recommended_track_indices = np.random.choice(num_tracks, size=num_recommendations, replace=False)\n",
    "\n",
    "# Create DataFrames for true and predicted labels\n",
    "true_labels_df = pd.DataFrame({'track_index': recommended_track_indices, 'true_label': true_labels[recommended_track_indices]})\n",
    "predicted_labels_df = pd.DataFrame({'track_index': indices[0], 'predicted_label': np.ones(num_recommendations)})  # Assuming all recommendations are relevant (1)\n",
    "\n",
    "# Merge DataFrames to have track information along with labels\n",
    "true_labels_df = pd.merge(true_labels_df, data[['track_name', 'artist(s)_name']], left_on='track_index', right_index=True)\n",
    "predicted_labels_df = pd.merge(predicted_labels_df, data[['track_name', 'artist(s)_name']], left_on='track_index', right_index=True)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"True Labels DataFrame:\")\n",
    "print(true_labels_df)\n",
    "\n",
    "print(\"\\nPredicted Labels DataFrame:\")\n",
    "print(predicted_labels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344845f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m predicted_labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, predicted_labels)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate F1 score\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:430\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    428\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    433\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6, 4]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Extract true and predicted labels\n",
    "true_labels = true_labels_df['true_label']\n",
    "predicted_labels = predicted_labels_df['predicted_label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(\"F1 score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f68500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "F1 score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Extract true and predicted labels\n",
    "true_labels = true_labels_df['true_label']\n",
    "predicted_labels = predicted_labels_df['predicted_label']\n",
    "\n",
    "# Ensure both DataFrames have the same number of samples\n",
    "min_samples = min(len(true_labels), len(predicted_labels))\n",
    "true_labels = true_labels[:min_samples]\n",
    "predicted_labels = predicted_labels[:min_samples]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2936fbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(true_labels, predicted_labels)\n",
    "print(\"R2 score:\", r2)\n",
    "\n",
    "#because of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195c1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
